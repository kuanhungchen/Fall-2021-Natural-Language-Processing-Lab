{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"0e56d558c252a4887cb9bc110bf182e2aa9946109639baab21a0ce4014d1d01c"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"109061539.ipynb","provenance":[{"file_id":"17xmJCylEB5gRAOMzqfKqqDi60GIa396i","timestamp":1631784466543}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"YZE5P94K7uu9"},"source":["# Assignment 01: N-gram Frequency\n","\n","During the natural language processing, sometimes we would like to know how many times a word, a phrase or a pattern has appeared in the text. This is called the **word/phrase frequency** .  \n","For example, in the sentence *\\\"The more you read, the better your vocabulary becomes\\\"*, the fequency of *read* is `1`, while the frequency of *the* is `2` .  \n","\n","Word frequency can be used to gain some knowledge from the article.  \n","As an example, if the frequency of the word *language* is much higher than other words in an article, we can assume that this article may be related to linguistic topics.  \n","\n","In this assignment, we will tell you what N-gram is and how to calculate N-gram frequency.  "]},{"cell_type":"markdown","metadata":{"id":"lM3B3zzR7uvC"},"source":["## Tokenization\n","\n","Before we start counting words, of course we need to transform the sentence into *words* .  \n","As what we want, **Tokenization** is the process to split a sentence into smaller word units, which we call *tokens* .  \n","\n","Take the sentence *\"Of course we're celebrating the New Year!\"* for example.  \n","We need to break it into a token list like `[\"Of\", \"course\", ...]` to begin counting.  \n","\n","In English, this could be easy because we can intuitively use spaces to seperate every words.  \n","> \"Of course we're celebrating the New Year!\"  \n","> -> [\"Of\", \"course\", <u>\"we're\"</u>, \"celebrating\", \"the\", \"New\", <u>\"Year!\"</u>]\n","\n","However, some tokenizer may also treat punctuations and abbreviations as independant units:  \n","> \"Of course we're celebrating the New Year!\"  \n","> -> [\"Of\", \"course\", <span style=\"color: red\"><u>\"we\", \"'\", \"re\"</u></span>, \"celebrating\", \"the\", \"New\", <span style=\"color: red\"><u>\"Year\", \"!\"</u></span>]\n","\n","Also, you might want to treat some special terms as single tokens:  \n","> \"Of course we're celebrating the New Year!\"  \n","> -> [\"Of\", \"course\", \"we're\", \"celebrating\", \"the\", <span style=\"color: red\"><u>\"New Year!\"</u></span>]\n","\n","And in Chinese this becomes even trickier without the hint from spaces:  \n","> \"今天天氣真好。\"  \n","> -> [\"今天\", \"天氣\", \"真\", \"好\", \"。\"]\n"]},{"cell_type":"markdown","metadata":{"id":"0fIS-N1o7uvD"},"source":["Knowing what tokenization is, now we can start to build our own counter!"]},{"cell_type":"markdown","metadata":{"id":"zIvxxPan7uvE"},"source":["**open the file**"]},{"cell_type":"code","metadata":{"id":"mskUnDsc7uvF"},"source":["import os\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXnuJqt87uvG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632117720705,"user_tz":-480,"elapsed":620,"user":{"displayName":"KH Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02519279833212751318"}},"outputId":"610470db-6f59-407a-adf0-4e606991a1f2"},"source":["file_path = os.path.join('data', 'big.txt') # change to where you put your data\n","with open(file_path, 'r') as f:\n","    text = f.read()\n","print(text[:90])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n","by Sir Arthur Conan Doyle\n"]}]},{"cell_type":"markdown","metadata":{"id":"kxnXawDl7uvH"},"source":["**<span style=\"color: red\">[ TODO ]</span> Implement your tokenization function here!**  \n","\n","In this assignment, we will use the following rules: \n"," 1. Ignore case (e.g., \"The\" is the same as \"the\")\n"," 2. Split by white spaces and punctuations\n"," 3. Ignore all punctuation\n","\n","Example:\n","`\"Hello! I'm your TA!\" -> [\"hello\", \"i\", \"m\", \"your\", \"ta\"]`  "]},{"cell_type":"code","metadata":{"id":"KcBUpBrT7uvH"},"source":["def tokenize(text):\n","    # [ TODO ] transform to lower case\n","    text = text.lower()\n","    # [ TODO ] seperate the words\n","    # tokens = [text]\n","    tokens = re.split('\\W+', text)\n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMEGqjAk7uvH"},"source":["**Test your function!**  \n"]},{"cell_type":"code","metadata":{"id":"C2RCQXIU7uvI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632117751269,"user_tz":-480,"elapsed":773,"user":{"displayName":"KH Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02519279833212751318"}},"outputId":"1dddd7b2-5763-43f9-f046-321bd2db3853"},"source":["tokens = tokenize(text)\n","# print(len(tokens))\n","print(tokens[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes']\n"]}]},{"cell_type":"markdown","metadata":{"id":"LIdAzKm97uvI"},"source":["<span style=\"color: green\">Expected output:</span> `['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes']`"]},{"cell_type":"markdown","metadata":{"id":"Gfdf-SMk7uvI"},"source":["## Frequency counting\n","We have splitted the sentence into tokens! Now we can try to calculate the frequency.  \n","<span style=\"color: red\">[ TODO ]</span> Try to <u>write a counter to sum up how many times the word shows up</u>.   \n","<small>(*Hint: dict, defaultdict, counter, etc.)</small>"]},{"cell_type":"code","metadata":{"id":"Pft2kpCi7uvI"},"source":["def calculate_frequency(tokens):\n","    # [ TODO ]\n","    # frequency = {...}\n","    import collections\n","    frequency = collections.Counter(tokens)\n","    \"\"\"\n","    Sample output: \n","    {\n","        'the': 79809, \n","        'project': 288,\n","        ...\n","    }\n","    \"\"\"\n","    return frequency"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"niRn4uXA7uvJ"},"source":["counter = calculate_frequency(tokens)\n","# for i, (k, v) in enumerate(counter.items()):\n","#     print(k, v)\n","#     if i == 10:\n","#         break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v0i1yFLy7uvJ"},"source":["Since there are too many entries in this counter, it may not be a good idea if we print them all.  \n","<span style=\"color: red\">[ TODO ]</span> Let's <u>write an utility function to print only the words with top-N frequency</u>. "]},{"cell_type":"code","metadata":{"id":"Vf0ykl0R7uvJ"},"source":["def print_top_n(counter, n=10):\n","    # [ TODO ]\n","    for k, v in sorted(counter.items(), key=lambda item: item[1], reverse=True)[:n]:\n","        print(k, v)\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rtd9q8Vn7uvJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632117772015,"user_tz":-480,"elapsed":317,"user":{"displayName":"KH Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02519279833212751318"}},"outputId":"8f095dcf-4d97-47f9-c388-275fd5673452"},"source":["print_top_n(counter, n=10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the 79809\n","of 40024\n","and 38312\n","to 28765\n","in 22023\n","a 21124\n","that 12512\n","he 12401\n","was 11410\n","it 10681\n"]}]},{"cell_type":"markdown","metadata":{"id":"L0VHN7IL7uvK"},"source":["<span style=\"color: green\">Expected output:</span>\n","```\n","the 79809\n","of 40024\n","and 38312\n","to 28765\n","in 22023\n","a 21124\n","that 12512\n","he 12401\n","was 11410\n","it 10681\n","```"]},{"cell_type":"markdown","metadata":{"id":"HFtBAswS7uvK"},"source":["## N-gram\n","\n","Now we have known how to calculate the word frequency.  \n","However, sometimes we not only want to know about single words, but also about the *phrases*, and here N-gram can be brought in.  \n","\n","**N-gram is a contiguous sequence of n items from a given sample of text or speech.** <small>(Definition from [wikipedia](https://en.wikipedia.org/wiki/N-gram))</small>  \n","\n","Consider the token list from previous exmaple: `[\"Of\", \"course\", \"we're\", \"celebrating\", \"the\", \"New\", \"Year!\"]` .  \n","The 2-gram, or bigram, of this example is `[\"Of course\", \"course we're\", \"we're celebrating\", ...]` .  \n","You may notice that the phrase \"of course\" now is bundled and counted together, and this is where N-gram has its power.  "]},{"cell_type":"markdown","metadata":{"id":"X3l2hUnt7uvK"},"source":["<span style=\"color: red\">[ TODO ]</span> In the following section, let's (1) <u>write a function to generate n-gram</u> and then (2) <u>calculate the frequency of grams</u> with different width.  "]},{"cell_type":"code","metadata":{"id":"RSR5gCSQ7uvL"},"source":["def get_ngram(tokens, n=2):\n","    # [ TODO ]\n","    ngram_tokens = []\n","    for i in range(len(tokens) - n):\n","        ngram_tokens.append(\" \".join(tokens[i:i + n]))\n","    return ngram_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWXUSbOM7uvL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632117787363,"user_tz":-480,"elapsed":1001,"user":{"displayName":"KH Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02519279833212751318"}},"outputId":"dab51c46-9039-4e36-b9da-571cf8afe497"},"source":["bigram = get_ngram(tokens, n=2)\n","print(bigram[:5])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['the project', 'project gutenberg', 'gutenberg ebook', 'ebook of', 'of the']\n"]}]},{"cell_type":"markdown","metadata":{"id":"y6RfyKRS7uvL"},"source":["<span style=\"color: green\">Expected output:</span> `['the project', 'project gutenberg', 'gutenberg ebook', 'ebook of', 'of the']`"]},{"cell_type":"markdown","metadata":{"id":"7okHHkLg7uvL"},"source":["After the 2-gram is generated, we can use the same counter we built above to count the frequency.  \n","<small>Note that if your counter couldn't work as expected here, you need to fix it above.</small>"]},{"cell_type":"code","metadata":{"id":"vQwZw4vz7uvL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632117803034,"user_tz":-480,"elapsed":315,"user":{"displayName":"KH Chen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02519279833212751318"}},"outputId":"c45265b9-3d1e-44ef-b586-c4020c1360f7"},"source":["bigram_counter = calculate_frequency(bigram)\n","print_top_n(bigram_counter)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["of the 12528\n","in the 6446\n","to the 4464\n","and the 3202\n","on the 2526\n","at the 2103\n","by the 1937\n","from the 1865\n","with the 1735\n","of a 1710\n"]}]},{"cell_type":"markdown","metadata":{"id":"kpp5K28-7uvM"},"source":["<span style=\"color: green\">Expected output:</span>\n","```\n","of the 12528\n","in the 6446\n","to the 4464\n","and the 3202\n","on the 2526\n","at the 2103\n","by the 1937\n","from the 1865\n","with the 1735\n","of a 1710\n","```"]},{"cell_type":"markdown","metadata":{"id":"8OUC0O497uvM"},"source":["## Conclusion\n","\n","Congraturation! You've succefully built a simple ngram frequency calculator! ;)  \n","Feel free to try building a trigram, 4-gram, etc. counter and observe the difference.  "]},{"cell_type":"markdown","metadata":{"id":"AQT_4yJ87uvM"},"source":["## TA's Notes\n","\n","Assignment#1 is just a warm-up practice, so you don't need to hand it in this week.  \n","However, **starting from the next week you'll need explain your implementation to TAs** after you finish your assignment.  \n","The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with TA before you miss the deadline**</u> .  \n","Note that **late submission will not be allowed**.  "]}]}