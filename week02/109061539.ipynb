{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "109061539.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsaHtMxBpkCW"
      },
      "source": [
        "# Assignment02: Find Academic Keyword List\n",
        "Academic Keywords are the words we seldom use ordinarily, but often use in Academic articles. \"This shows\" and \"in conclusion\" are examples of Academic Keywords. This assignment want you to use Rank Ratio and compare two dataset to find Academic Keyword List(AKL).\n",
        "<br><br>\n",
        "One dataset is taken from [`British Academic Written English Corpus(BAWE)`](https://www.coventry.ac.uk/research/research-directories/current-projects/2015/british-academic-written-english-corpus-bawe/), which collect a record of proficient university-level student writing. Hence, BAWE can be seen as Academic data. Another one is called [`Web1T`](https://catalog.ldc.upenn.edu/LDC2006T13), which is presented by Google. Web1T colloct 1 trillion words of English Web, so we can treat it as the representative of common words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5uLs-Z4pkCb"
      },
      "source": [
        "## N-gram counting\n",
        "This part is almost same as what you need to do in Assignment01. The way to find N-gram is the same as Assignment01. However, tokenization and calculating frequency are a little different. The rules of tokenization in this Assignment are:\n",
        " 1. Ignore case (e.g., \"The\" is the same as \"the\")\n",
        " 2. Split by white spaces <s>and punctuations</s>\n",
        " 3. Ignore all punctuation\n",
        "<br><br>\n",
        "\n",
        "As for calculating frequency, we want you calculating <u>document frequency</u> in this Assignment. <br>What is document frequency? \n",
        "<br>Article 1: \n",
        "> We all know that water masses in the ocean are thought to be transferred by the wind. ...\n",
        "\n",
        "Althought there are at least 2 \"the\" in Article 1, the document frequency of \"the\" is still 1 in this article.<br> No mater how many times does \"the\" show up in Article 1, the document frquency of it is always 1.<br>\n",
        "Article 2: \n",
        "> The film Dantes Peak is about a dormant volcano that suddenly erupts and threatens the nearby town. ...\n",
        "\n",
        "Considering the Article 1 and 2, the document frequency of \"the\" is 2 now.<br>\n",
        "Document frequency can reduce the influence of terms, like \"NLP\".\n",
        "<br><br>\n",
        "<span style=\"color: red\">[ TODO ]</span> Try to modify the functions coded in Assignment01 to <u>calculate document frequencies of all unigram.</u>.\n",
        "\n",
        "Google has calculated the frequency of N-gram, so you only need to do it on BAWE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYOAHy058B60"
      },
      "source": [
        "'''\n",
        "I do the assignments on Google Colab, so this section just helps me mounting\n",
        "Google Drive to Colab directories.\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/week02/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAvwpUzzpkCd"
      },
      "source": [
        "def tokenize(text):\n",
        "    import re\n",
        "\n",
        "    # [ TODO ] transform to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # ignore punctuations\n",
        "    '''\n",
        "    Originally, I deleted all punctuations from:\n",
        "    -->  \",.'[]\\\"/-():;%=<>?!*~|&+{_}\\@$#`^\"  <--\n",
        "\n",
        "    The rank ratio seems not very similar to TA's sample.\n",
        "    So I tried a lot of combinations of punctuations and finally got a quite\n",
        "    good one:\n",
        "    --> \"=<>?!*~|&+{_}\\@$#`^\" <--\n",
        "    (What I mean good is about the rank ratio, I think it should be similar to\n",
        "    the sample).\n",
        "    '''\n",
        "    puncs = \"=<>?!*~|&+{_}\\@$#`^\"\n",
        "    for punc in puncs:\n",
        "        text = text.replace(punc, '')\n",
        "    \n",
        "    # [ TODO ] seperate the words\n",
        "    tokens = re.split('\\s+', text)\n",
        "    '''\n",
        "    In assignment 1, I use \\W+ to separate words by any non-English and\n",
        "    non-numeric charaters.\n",
        "    But now we only split tokens by spaces, so I change it to \\s+ which contains\n",
        "    spaces, tabs, newlines, etc.\n",
        "    '''\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXBj0W4_pkCe"
      },
      "source": [
        "def calculate_frequency(tokens):\n",
        "    # [ TODO ]\n",
        "    # frequency = {...}\n",
        "    \n",
        "    frequency = {token: 1 for token in set(tokens)}\n",
        "    '''\n",
        "    Here we care about DOCUMENT FREQUENCY, meaning that words appearing more\n",
        "    than once can only be counted as one (in a single document).\n",
        "    '''\n",
        "    return frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahX6DjZspkCf"
      },
      "source": [
        "def get_ngram(tokens, n=2):\n",
        "    # [ TODO ]\n",
        "    ngram_tokens = []\n",
        "    for i in range(len(tokens) - n):\n",
        "        ngram_tokens.append(\" \".join(tokens[i:i + n]))\n",
        "    return ngram_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbBQa3H5pkCg"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS9X7BpapkCg"
      },
      "source": [
        "file_path = os.path.join('data', 'BAWE.txt')\n",
        "BAWE_unigram = []\n",
        "#### [ TODO ] calculate document frequency of unigram in BAWE\n",
        "import collections\n",
        "BAWE_unigram_set = set()\n",
        "BAWE_unigram_frequency = collections.defaultdict(int)\n",
        "'''\n",
        "I use BAWE_unigram_set to collect all unique unigrams, and use\n",
        "BAWE_unigram_frequency to accumulate each token with its document frequencies.\n",
        "'''\n",
        "with open(file_path, 'r') as f:\n",
        "    for text in f.readlines():\n",
        "        tokens = tokenize(text)\n",
        "        BAWE_unigram_set = BAWE_unigram_set.union(set(tokens))\n",
        "        doc_frequency = calculate_frequency(tokens)\n",
        "        for k, v in doc_frequency.items():\n",
        "            BAWE_unigram_frequency[k] += v\n",
        "BAWE_unigram = list(BAWE_unigram_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npN1SooRpkCh"
      },
      "source": [
        "# Read Web1T Data\n",
        "file_path = os.path.join('data', 'Web1T_unigram.txt')\n",
        "Web1T_unigram_counter = {}\n",
        "with open(file_path,'r') as f:\n",
        "    for line in f.readlines():\n",
        "        line=line.rstrip(\"\\n\").split(\"\\t\")\n",
        "        Web1T_unigram_counter[line[0]] = int(line[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh7oIpe2pkCi"
      },
      "source": [
        "## Rank\n",
        "Rank unigrms by their frequencies. The higher the frequency, the higher the rank.(The most frequent unigram ranks 1.)<br>\n",
        "<span style=\"color: red\">[ TODO ]</span> <u>Rank unigrams for Web1T and BAWE.</u>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9CuNuC7pkCi"
      },
      "source": [
        "Web1T_unigram_Rank = {}\n",
        "#### [ TODO ] Rank unigrams for Web1T\n",
        "'''\n",
        "I found out that if I give the ranks one by one (e.g. 1, 2, 3, ...) without\n",
        "caring unigrams with duplicate frequencies, the rank ratios are not accurate.\n",
        "Only when I take account of duplicate frequencies, meaning that same frequencies\n",
        "should have same ranks, the rank ratios just become more accurate a bit.\n",
        "'''\n",
        "prev_freq = -1\n",
        "rank = 0\n",
        "acc = 1\n",
        "for k, v in sorted(Web1T_unigram_counter.items(), key=lambda elem: elem[1], reverse=True):\n",
        "    if prev_freq != v:\n",
        "        rank += acc\n",
        "        acc = 1\n",
        "    else:\n",
        "        acc += 1\n",
        "    Web1T_unigram_Rank[k] = rank\n",
        "    prev_freq = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZTxrjenpkCi"
      },
      "source": [
        "BAWE_unigram_Rank = {}\n",
        "#### [ TODO ] Rank unigrams for BAWE\n",
        "prev_freq = -1\n",
        "rank = 0\n",
        "acc = 1\n",
        "for k, v in sorted(BAWE_unigram_frequency.items(), key=lambda elem: elem[1], reverse=True):\n",
        "    if prev_freq != v:\n",
        "        rank += acc\n",
        "        acc = 1\n",
        "    else:\n",
        "        acc += 1\n",
        "    BAWE_unigram_Rank[k] = rank\n",
        "    prev_freq = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PTMOAq0pkCj"
      },
      "source": [
        "## Calculate Rank Ratio\n",
        "In this step, you need to map the same unigram in two dataset, and caalculate the Rank Ratio of unigram in BAWE.  <br>Please follow the formula for calculating Rank Ratio:<br> \n",
        "<br>\n",
        "<img src=\"https://imgur.com/vmK7Q1K.jpg\" width=30%><br>\n",
        "If the unigram doesn't appear in Web1T, the rank of it is treated as 1.\n",
        "\n",
        "<span style=\"color: red\">[ TODO ]</span> Please calculate all rank ratios of unigrams in BAWE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbNlnHaypkCj"
      },
      "source": [
        "#### [TODO] calculate all rank ratios of unigrams in BAWE\n",
        "BAWE_unigram_rank_ratio = {}\n",
        "for unigram in BAWE_unigram:\n",
        "    BAWE_unigram_rank_ratio[unigram] = Web1T_unigram_Rank.get(unigram, 1) / BAWE_unigram_Rank[unigram]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn1BPU09pkCj"
      },
      "source": [
        "## sort the result\n",
        "<span style=\"color: red\">[ TODO ]</span> Please show top 30 uigrams in Rank Ratio and the value of their Rank Ratio in this format: \n",
        "<br>\n",
        "<img src=\"https://imgur.com/AEkiCRr.jpg\" width=50%>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5v62G0hpkCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586993f9-1d0e-4975-c580-310ab3f1d4af"
      },
      "source": [
        "#### [ TODO ] show the result\n",
        "print(\"rank\\tunigram\\t\\t\\tRank Ratio\")\n",
        "for i, (k, v) in enumerate(sorted(BAWE_unigram_rank_ratio.items(), key=lambda elem: elem[1], reverse=True)[:30], start=1):\n",
        "    print(\"{}{}{}\".format(str(i).ljust(8, ' '), str(k).ljust(24, ' '), str(round(v, 4)).ljust(16, ' ')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank\tunigram\t\t\tRank Ratio\n",
            "1       cannot                  666.2188  \n",
            "2       conclusion              19.0425   \n",
            "3       whilst                  17.126    \n",
            "4       emphasises              16.6271   \n",
            "5       trudgill                16.06     \n",
            "6       sibilance               15.9606   \n",
            "7       suggests                15.9207   \n",
            "8       argued                  15.5319   \n",
            "9       generalisability        15.4392   \n",
            "10      essay                   14.7581   \n",
            "11      legitimising            14.6206   \n",
            "12      argues                  14.3983   \n",
            "13      therefore               14.163    \n",
            "14      plosives                14.0546   \n",
            "15      analysing               13.676    \n",
            "16      foregrounded            13.4585   \n",
            "17      firstly                 13.2532   \n",
            "18      hypothesise             13.1172   \n",
            "19      emphasising             13.0346   \n",
            "20      analyse                 12.918    \n",
            "21      stoppered               12.9052   \n",
            "22      analysed                12.8473   \n",
            "23      hypothesised            12.6997   \n",
            "24      behaviourist            12.6893   \n",
            "25      pipetted                12.5794   \n",
            "26      assonance               12.1747   \n",
            "27      homogenised             11.6749   \n",
            "28      argue                   11.6256   \n",
            "29      criticised              11.3956   \n",
            "30      ethnocentric            11.3813   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0iC57SJpkCk"
      },
      "source": [
        "## for Bigrams\n",
        "<span style=\"color: red\">[ TODO ]</span> Do the Same Thing for Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLiML6nm_4eg"
      },
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    # transform to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # ignore punctuations\n",
        "    puncs = \",.'[]\\\"/-():;%=?!*~|&+{_}\\@$#`^\"\n",
        "    for punc in puncs:\n",
        "        text = text.replace(punc, '')\n",
        "    \n",
        "    # seperate the words\n",
        "    tokens = re.split('\\s+', text)\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7IMrr_spkCk"
      },
      "source": [
        "file_path = os.path.join('data', 'BAWE.txt')\n",
        "BAWE_bigram = []\n",
        "# calculate document frequency of bigram in BAWE\n",
        "import collections\n",
        "BAWE_bigram_set = set()\n",
        "BAWE_bigram_frequency = collections.defaultdict(int)\n",
        "with open(file_path, 'r') as f:\n",
        "    for text in f.readlines():\n",
        "        tokens = tokenize(text)\n",
        "        bigram = get_ngram(tokens, n=2)  # get bigrams\n",
        "        BAWE_bigram_set = BAWE_bigram_set.union(set(bigram))\n",
        "        doc_frequency = calculate_frequency(bigram)\n",
        "        for k, v in doc_frequency.items():\n",
        "            BAWE_bigram_frequency[k] += v\n",
        "BAWE_bigram = list(BAWE_bigram_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1Nyk_zP2bwh"
      },
      "source": [
        "# read Web1T bigram data\n",
        "file_path = os.path.join('data', 'Web1T_bigram.txt')\n",
        "Web1T_bigram_counter = {}\n",
        "with open(file_path,'r') as f:\n",
        "    for line in f.readlines():\n",
        "        line = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "        Web1T_bigram_counter[line[0]] = int(line[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fFnqItLf8gh"
      },
      "source": [
        "# calculate rank of bigram in Web1T\n",
        "Web1T_bigram_Rank = {}\n",
        "prev_freq = -1\n",
        "rank = 0\n",
        "acc = 1\n",
        "for k, v in sorted(Web1T_bigram_counter.items(), key=lambda elem: elem[1], reverse=True):\n",
        "    if prev_freq != v:\n",
        "        rank += acc\n",
        "        acc = 1\n",
        "    else:\n",
        "        acc += 1\n",
        "    Web1T_bigram_Rank[k] = rank\n",
        "    prev_freq = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRwINEK-2hE2"
      },
      "source": [
        "## calculate rank of bigram in BAWE\n",
        "BAWE_bigram_Rank = {}\n",
        "prev_freq = -1\n",
        "rank = 0\n",
        "acc = 1\n",
        "for k, v in sorted(BAWE_bigram_frequency.items(), key=lambda elem: elem[1], reverse=True):\n",
        "    if prev_freq != v:\n",
        "        rank += acc\n",
        "        acc = 1\n",
        "    else:\n",
        "        acc += 1\n",
        "    BAWE_bigram_Rank[k] = rank\n",
        "    prev_freq = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao4kAkdUf-fr"
      },
      "source": [
        "# calculate rank ratio of bigram in BAWE\n",
        "BAWE_bigram_rank_ratio = {}\n",
        "for bigram in BAWE_bigram:\n",
        "    BAWE_bigram_rank_ratio[bigram] = Web1T_bigram_Rank.get(bigram, 1) / BAWE_bigram_Rank[bigram]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy2FnFgC7IdL",
        "outputId": "c4e68a10-ba2d-4f9a-db6d-281bd142c2e4"
      },
      "source": [
        "# print results\n",
        "print(\"rank\\tbigram\\t\\t\\tRank Ratio\")\n",
        "for i, (k, v) in enumerate(sorted(BAWE_bigram_rank_ratio.items(), key=lambda elem: elem[1], reverse=True)[:30], start=1):\n",
        "    print(\"{}{}{}\".format(str(i).ljust(8, ' '), str(k).ljust(24, ' '), str(round(v, 4)).ljust(16, ' ')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank\tbigram\t\t\tRank Ratio\n",
            "1       in conclusion           517.3043  \n",
            "2       however this            418.7165  \n",
            "3       however the             377.0602  \n",
            "4       however in              285.8129  \n",
            "5       however it              247.1333  \n",
            "6       this essay              228.844   \n",
            "7       however there           218.7548  \n",
            "8       the british             186.9405  \n",
            "9       the european            155.2256  \n",
            "10      this suggests           139.9115  \n",
            "11      this shows              107.3271  \n",
            "12      analysis the            93.0619   \n",
            "13      therefore it            91.062    \n",
            "14      however a               90.7214   \n",
            "15      see appendix            90.5267   \n",
            "16      therefore the           87.2732   \n",
            "17      method the              77.1569   \n",
            "18      conclusion in           65.8483   \n",
            "19      however he              63.551    \n",
            "20      the uk                  63.2821   \n",
            "21      however to              62.0589   \n",
            "22      the united              61.6882   \n",
            "23      a persons               61.066    \n",
            "24      system the              60.4528   \n",
            "25      therefore this          59.5873   \n",
            "26      i shall                 58.8533   \n",
            "27      example it              58.8259   \n",
            "28      despite this            58.0743   \n",
            "29      development the         57.5143   \n",
            "30      in england              57.3901   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6-8QfA6NpBP"
      },
      "source": [
        "### Optional Problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF0Tc-FEVMJt",
        "outputId": "0c4d3e96-7dbb-49a1-ffd6-63a326918747"
      },
      "source": [
        "# read all AKL words\n",
        "AKL = \"ability, absence, account, achievement, act, action, activity, addition, adoption, adult, advance, advantage, advice, age, aim, alternative, amount, analogy, analysis, application, approach, argument, aspect, assertion, assessment, assistance, association, assumption, attempt, attention, attitude, author, awareness, balance, basis, behaviour, being, belief, benefit, bias, birth, capacity, case, category, cause, centre, challenge, change, character, characteristic, choice, circumstance, class, classification, code, colleague, combination, commitment, committee, communication, community, comparison, complexity, compromise, concentration, concept, conception, concern, conclusion, condition, conduct, conflict, consensus, consequence, consideration, constraint, construction, content, contradiction, contrast, contribution, control, convention, correlation, country, creation, crisis, criterion, criticism, culture, damage, data, debate, decision, decline, defence, definition, degree, demand, description, destruction, determination, development, difference, difficulty, dilemma, dimension, disadvantage, discovery, discrimination, discussion, distinction, diversity, division, doctrine, effect, effectiveness, element, emphasis, environment, error, essence, establishment, evaluation, event, evidence, evolution, examination, example, exception, exclusion, existence, expansion, experience, experiment, explanation, exposure, extent, extreme, fact, factor, failure, feature, female, figure, finding, force, form, formation, function, future, gain, group, growth, guidance, guideline, hypothesis, idea, identity, impact, implication, importance, improvement, increase, indication, individual, influence, information, insight, instance, institution, integration, interaction, interest, interpretation, intervention, introduction, investigation, isolation, issue, kind, knowledge, lack, learning, level, likelihood, limit, limitation, link, list, literature, logic, loss, maintenance, majority, male, manipulation, mankind, material, means, measure, medium, member, method, minority, mode, model, motivation, movement, need, network, norm, notion, number, observation, observer, occurrence, operation, opportunity, option, organisation, outcome, output, parallel, parent, part, participant, past, pattern, percentage, perception, period, person, personality, perspective, phenomenon, point, policy, population, position, possibility, potential, practice, presence, pressure, problem, procedure, process, production, programme, progress, property, proportion, proposition, protection, provision, publication, purpose, quality, question, range, rate, reader, reality, reason, reasoning, recognition, reduction, reference, relation, relationship, relevance, report, representative, reproduction, requirement, research, resistance, resolution, resource, respect, restriction, result, review, rise, risk, role, rule, sample, scale, scheme, scope, search, section, selection, sense, separation, series, service, set, sex, shift, significance, similarity, situation, skill, society, solution, source, space, spread, standard, statistics, stimulus, strategy, stress, structure, subject, success, summary, support, survey, system, target, task, team, technique, tendency, tension, term, theme, theory, tolerance, topic, tradition, transition, trend, type, uncertainty, understanding, unit, use, validity, value, variation, variety, version, view, viewpoint, volume, whole, work, world, accept, account (for), achieve, acquire, act, adapt, adopt, advance, advocate, affect, aid, aim, allocate, allow, alter, analyse, appear, apply, argue, arise, assert, assess, assign, associate, assist, assume, attain, attempt, attend, attribute, avoid, base, be, become, benefit, can, cause, characterise, choose, cite, claim, clarify, classify, coincide, combine, compare, compete, comprise, concentrate, concern, conclude, conduct, confine, conform, connect, consider, consist, constitute, construct, contain, contrast, contribute, control, convert, correspond, create, damage, deal, decline, define, demonstrate, depend, derive, describe, design, destroy, determine, develop, differ, differentiate, diminish, direct, discuss, display, distinguish, divide, dominate, effect, eliminate, emerge, emphasize, employ, enable, encounter, encourage, enhance, ensure, establish, evaluate, evolve, examine, exceed, exclude, exemplify, exist, expand, experience, explain, expose, express, extend, facilitate, fail, favour, finance, focus, follow, form, formulate, function, gain, generate, govern, highlight, identify, illustrate, imply, impose, improve, include, incorporate, increase, indicate, induce, influence, initiate, integrate, interpret, introduce, investigate, involve, isolate, label, lack, lead, limit, link, locate, maintain, may, measure, neglect, note, obtain, occur, operate, outline, overcome, participate, perceive, perform, permit, pose, possess, precede, predict, present, preserve, prevent, produce, promote, propose, prove, provide, publish, pursue, quote, receive, record, reduce, refer, reflect, regard, regulate, reinforce, reject, relate, rely, remain, remove, render, replace, report, represent, reproduce, require, resolve, respond, restrict, result, retain, reveal, seek, select, separate, should, show, solve, specify, state, stimulate, strengthen, stress, study, submit, suffer, suggest, summarise, supply, support, sustain, tackle, tend, term, transform, treat, undermine, undertake, use, vary, view, write, yield, absolute, abstract, acceptable, accessible, active, actual, acute, additional, adequate, alternative, apparent, applicable, appropriate, arbitrary, available, average, basic, central, certain, clear, common, competitive, complete, complex, comprehensive, considerable, consistent, conventional, correct, critical, crucial, dependent, detailed, different, difficult, distinct, dominant, early, effective, equal, equivalent, essential, evident, excessive, experimental, explicit, extensive, extreme, far, favourable, final, fixed, following, formal, frequent, fundamental, future, general, great, high, human, ideal, identical, immediate, important, inadequate, incomplete, independent, indirect, individual, inferior, influential, inherent, initial, interesting, internal, large, late, leading, likely, limited, local, logical, main, major, male, maximum, mental, minimal, minor, misleading, modern, mutual, natural, necessary, negative, new, normal, obvious, original, other, overall, parallel, partial, particular, passive, past, permanent, physical, positive, possible, potential, practical, present, previous, primary, prime, principal, productive, profound, progressive, prominent, psychological, radical, random, rapid, rational, real, realistic, recent, related, relative, relevant, representative, responsible, restricted, scientific, secondary, selective, separate, severe, sexual, significant, similar, simple, single, so-called, social, special, specific, stable, standard, strict, subsequent, substantial, successful, successive, sufficient, suitable, surprising, symbolic, systematic, theoretical, total, traditional, true, typical, unique, unlike, unlikely, unsuccessful, useful, valid, valuable, varied, various, visual, vital, wide, widespread, above, accordingly, accurately, adequately, also, approximately, at best, basically, clearly, closely, commonly, consequently, considerably, conversely, correctly, directly, effectively, e.g., either, equally, especially, essentially, explicitly, extremely, fairly, far, for example, for instance, frequently, fully, further, generally, greatly, hence, highly, however, increasingly, indeed, independently, indirectly, inevitably, initially, in general, in particular, largely, less, mainly, more, moreover, most, namely, necessarily, normally, notably, often, only, originally, over, partially, particularly, potentially, previously, primarily, purely, readily, recently, relatively, secondly, significantly, similarly, simply, socially, solely somewhat, specifically, strongly, subsequently, successfully, thereby, therefore, thus, traditionally, typically, ultimately, virtually, wholly, widely, according to, although, an, as, as opposed to, as to, as well as, because, because of, between, both, by, contrary to, depending on, despite, due to, during, each, even though, fewer, first, former, from, for, given that, in, in addition to, in common with, in favour of, in relation to, in response to, in terms of, in that, in the light of, including, its, itself, latter, less, little, many, most, of, or, other than, per, prior to, provided, rather than, same, second, several, since, some, subject to, such, such as, than, that, the, their, themselves, these, third, this, those, to, unlike, upon, versus, whereas, whether, whether or not, which, within\".split(\", \")\n",
        "print(AKL[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ability', 'absence', 'account', 'achievement', 'act', 'action', 'activity', 'addition', 'adoption', 'adult']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51n34140NoKg",
        "outputId": "768188bd-511b-4d9a-f679-e93d0807f161"
      },
      "source": [
        "def tokenize(text):\n",
        "    import re\n",
        "\n",
        "    # transform to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # ignore punctuations\n",
        "    puncs = \",.'[]\\\"/-():;%=<>?!*~|&+{_}\\@$#`^\"\n",
        "    for punc in puncs:\n",
        "        text = text.replace(punc, '')\n",
        "    \n",
        "    # seperate the words\n",
        "    tokens = re.split('\\s+', text)\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "def get_ngram(tokens, n=2):\n",
        "    ngram_tokens = []\n",
        "    for i in range(len(tokens) - n):\n",
        "        ngram_tokens.append(\" \".join(tokens[i:i + n]))\n",
        "    return ngram_tokens\n",
        "\n",
        "def calculate_frequency(tokens):\n",
        "    import collections\n",
        "\n",
        "    frequency = collections.Counter(tokens)\n",
        "    return frequency\n",
        "\n",
        "def calculate_rank(freqs):\n",
        "    ranks = {}\n",
        "    prev_freq = -1\n",
        "    rank = 0\n",
        "    acc = 1\n",
        "    for k, v in sorted(freqs.items(), key=lambda elem: elem[1], reverse=True):\n",
        "        if prev_freq != v:\n",
        "            rank += acc\n",
        "            acc = 1\n",
        "        else:\n",
        "            acc += 1\n",
        "        ranks[k] = rank\n",
        "        prev_freq = v\n",
        "    return ranks\n",
        "\n",
        "def calculate_rank_ratio(ranks1, ranks2, ngrams):\n",
        "    # rank ratio = ranks1[ngram] / ranks2[ngram]\n",
        "    rank_ratios = {}\n",
        "    for ngram in ngrams:\n",
        "        rank_ratios[ngram] = ranks1.get(ngram, 1) / ranks2[ngram]\n",
        "    return rank_ratios\n",
        "\n",
        "\n",
        "s = \"Question Answering (QA) has been an area of active research. \\\n",
        "Recently, the state-of-the-art in QA research has been \\\n",
        "represented in the Text Retrieval Evaluation Conference (TREC) \\\n",
        "question answering track evaluations [Voorhees 2001], which \\\n",
        "involve retrieving a short (50 bytes long) answer to a set of test \\\n",
        "questions. In our work we address an aspect of question \\\n",
        "answering that was not a direct focus of the TREC QA track. We \\\n",
        "also consider a more general class of questions, where the \\\n",
        "answers may not be short, precise facts, and the user might be \\\n",
        "interested in multiple answers (e.g., consider the question “What \\\n",
        "are ways people can be motivated?”).\"\n",
        "\n",
        "\n",
        "### Unigram \n",
        "unigram_tokens = tokenize(s)\n",
        "unigrams = set(unigram_tokens)\n",
        "unigram_freqs = calculate_frequency(unigram_tokens)\n",
        "unigram_ranks = calculate_rank(unigram_freqs)\n",
        "unigram_rank_ratios = calculate_rank_ratio(Web1T_unigram_Rank, unigram_ranks, unigrams)\n",
        "\n",
        "print(\"Unigram\\nrank\\tunigram\\t\\t\\tRank Ratio\\tIs In AKL\")\n",
        "target_words = ['question', 'answer', 'area', 'active', 'research']\n",
        "for i, (k, v) in enumerate(sorted(unigram_rank_ratios.items(), key=lambda elem: elem[1], reverse=True), start=1):\n",
        "    if k in target_words:\n",
        "        print(\"{}{}{}{}\".format(str(i).ljust(8, ' '), str(k).ljust(24, ' '), str(round(v, 4)).ljust(16, ' '), str('YES' if k in AKL else 'NO')))\n",
        "\n",
        "### Bigram 109061539\n",
        "bigram_tokens = get_ngram(unigram_tokens, n=2)\n",
        "bigrams = set(bigram_tokens)\n",
        "bigram_freqs = calculate_frequency(bigram_tokens)\n",
        "bigram_ranks = calculate_rank(bigram_freqs)\n",
        "bigram_rank_ratios = calculate_rank_ratio(Web1T_bigram_Rank, bigram_ranks, bigrams)\n",
        "print(\"\\n\\nBigram\\nrank\\tbigram\\t\\t\\tRank Ratio\")\n",
        "for i, (k, v) in enumerate(sorted(bigram_rank_ratios.items(), key=lambda elem: elem[1], reverse=True)[:30], start=1):\n",
        "    print(\"{}{}{}\".format(str(i).ljust(8, ' '), str(k).ljust(24, ' '), str(round(v, 4)).ljust(16, ' ')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram\n",
            "rank\tunigram\t\t\tRank Ratio\tIs In AKL\n",
            "12      question                197.3333        YES\n",
            "24      answer                  59.5714         NO\n",
            "28      active                  45.4286         YES\n",
            "33      research                21.3333         YES\n",
            "37      area                    12.0476         NO\n",
            "\n",
            "\n",
            "Bigram\n",
            "rank\tbigram\t\t\tRank Ratio\n",
            "1       active research         90625.6667\n",
            "2       address an              85560.6667\n",
            "3       test questions          54331.6667\n",
            "4       we address              43644.6667\n",
            "5       of question             35411.0   \n",
            "6       recently the            34935.0   \n",
            "7       which involve           34121.0   \n",
            "8       are ways                27116.3333\n",
            "9       an aspect               24734.3333\n",
            "10      be short                24589.6667\n",
            "11      work we                 16651.0   \n",
            "12      also consider           14868.0   \n",
            "13      of test                 9011.0    \n",
            "14      research has            7112.6667 \n",
            "15      questions in            6733.0    \n",
            "16      in multiple             6715.3333 \n",
            "17      of active               6604.6667 \n",
            "18      more general            5840.0    \n",
            "19      consider a              4985.0    \n",
            "20      represented in          4913.0    \n",
            "21      facts and               4719.3333 \n",
            "22      our work                4227.0    \n",
            "23      the answers             3595.3333 \n",
            "24      of questions            3590.0    \n",
            "25      people can              2543.0    \n",
            "26      been an                 2468.6667 \n",
            "27      focus of                2310.6667 \n",
            "28      we also                 2130.3333 \n",
            "29      be interested           1963.0    \n",
            "30      a direct                1678.0    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPp9F4UFpkCk"
      },
      "source": [
        "## TA's Notes\n",
        "\n",
        "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=40492256) to reserve demo time.  \n",
        "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to eeclass. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
        "<br>Note that **late submission will not be allowed**.  "
      ]
    }
  ]
}